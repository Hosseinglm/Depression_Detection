{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1324a0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import Word2Vec, FastText, KeyedVectors\n",
    "from sklearn.utils import resample\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Bidirectional, LSTM, Dense\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b33ae03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>TITLE</th>\n",
       "      <th>DATE</th>\n",
       "      <th>INFO</th>\n",
       "      <th>TEXT</th>\n",
       "      <th>depression</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>subject1257</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-05-08 03:44:04</td>\n",
       "      <td>reddit post</td>\n",
       "      <td>Thank you for taking the time to answer with t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>subject1257</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-05-06 17:40:31</td>\n",
       "      <td>reddit post</td>\n",
       "      <td>Ugh, I'm sorry Jina... We also got bad news re...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>subject1257</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-05-06 17:33:43</td>\n",
       "      <td>reddit post</td>\n",
       "      <td>Thanks, I'm just looking forward to answers, w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>subject1257</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-05-06 16:42:28</td>\n",
       "      <td>reddit post</td>\n",
       "      <td>That's what we're here for!! Enjoy :) I actual...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>subject1257</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-05-06 16:02:41</td>\n",
       "      <td>reddit post</td>\n",
       "      <td>Well, officially on CD 1 of cycle 5, 9 months ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1076577</th>\n",
       "      <td>train_subject9974</td>\n",
       "      <td>Favorite SciFi ride?</td>\n",
       "      <td>2014-04-22 17:49:27</td>\n",
       "      <td>reddit post</td>\n",
       "      <td>What's your favorite vehicle of any kind from ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1076578</th>\n",
       "      <td>train_subject9974</td>\n",
       "      <td>Why did Kane go from demon spawn to corporate ...</td>\n",
       "      <td>2014-04-22 17:35:18</td>\n",
       "      <td>reddit post</td>\n",
       "      <td>Back in the day, Kane could barely speak. He w...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1076579</th>\n",
       "      <td>train_subject9974</td>\n",
       "      <td>Favorite comic book easter egg?</td>\n",
       "      <td>2014-04-20 15:21:57</td>\n",
       "      <td>reddit post</td>\n",
       "      <td>In honor of easter, what's your favorite easte...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1076580</th>\n",
       "      <td>train_subject9974</td>\n",
       "      <td>Recommended Dark Horse reading?</td>\n",
       "      <td>2014-04-19 17:40:11</td>\n",
       "      <td>reddit post</td>\n",
       "      <td>I've never really read many Dark Horse titles,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1076581</th>\n",
       "      <td>train_subject9974</td>\n",
       "      <td>Alan Moore is living the dream.</td>\n",
       "      <td>2014-04-19 15:43:16</td>\n",
       "      <td>reddit post</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1076582 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        ID                                              TITLE  \\\n",
       "0              subject1257                                                NaN   \n",
       "1              subject1257                                                NaN   \n",
       "2              subject1257                                                NaN   \n",
       "3              subject1257                                                NaN   \n",
       "4              subject1257                                                NaN   \n",
       "...                    ...                                                ...   \n",
       "1076577  train_subject9974                               Favorite SciFi ride?   \n",
       "1076578  train_subject9974  Why did Kane go from demon spawn to corporate ...   \n",
       "1076579  train_subject9974                    Favorite comic book easter egg?   \n",
       "1076580  train_subject9974                    Recommended Dark Horse reading?   \n",
       "1076581  train_subject9974                    Alan Moore is living the dream.   \n",
       "\n",
       "                        DATE         INFO  \\\n",
       "0        2017-05-08 03:44:04  reddit post   \n",
       "1        2017-05-06 17:40:31  reddit post   \n",
       "2        2017-05-06 17:33:43  reddit post   \n",
       "3        2017-05-06 16:42:28  reddit post   \n",
       "4        2017-05-06 16:02:41  reddit post   \n",
       "...                      ...          ...   \n",
       "1076577  2014-04-22 17:49:27  reddit post   \n",
       "1076578  2014-04-22 17:35:18  reddit post   \n",
       "1076579  2014-04-20 15:21:57  reddit post   \n",
       "1076580  2014-04-19 17:40:11  reddit post   \n",
       "1076581  2014-04-19 15:43:16  reddit post   \n",
       "\n",
       "                                                      TEXT  depression  \n",
       "0        Thank you for taking the time to answer with t...           1  \n",
       "1        Ugh, I'm sorry Jina... We also got bad news re...           1  \n",
       "2        Thanks, I'm just looking forward to answers, w...           1  \n",
       "3        That's what we're here for!! Enjoy :) I actual...           1  \n",
       "4        Well, officially on CD 1 of cycle 5, 9 months ...           1  \n",
       "...                                                    ...         ...  \n",
       "1076577  What's your favorite vehicle of any kind from ...           0  \n",
       "1076578  Back in the day, Kane could barely speak. He w...           0  \n",
       "1076579  In honor of easter, what's your favorite easte...           0  \n",
       "1076580  I've never really read many Dark Horse titles,...           0  \n",
       "1076581                                                NaN           0  \n",
       "\n",
       "[1076582 rows x 6 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('eRisk_DP_Dataset.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c3721d",
   "metadata": {},
   "source": [
    "# Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4648161",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4e84d2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " .:. Data Cleaning Done .:.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "def clean_text(text):\n",
    "    # Check if the input is a string\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"  # or return text if you want to keep the original value\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    # Remove numbers and special characters\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove extra whitespace\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "df['TEXT'] = df['TEXT'].apply(clean_text)\n",
    "print(' .:. Data Cleaning Done .:.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a2f0d7",
   "metadata": {},
   "source": [
    "### Data Transformation, Stemming and Tokenization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c0df703",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/hossein.glm/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/hossein.glm/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " .:. Data Transformation, Stemming and Tokenization are Done .:.\n"
     ]
    }
   ],
   "source": [
    "import regex as re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Downloading stopwords and wordnet data\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def advanced_transform_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Expand contractions (e.g., \"don't\" -> \"do not\")\n",
    "    contractions = {\n",
    "        \"ain't\": \"am not\",\n",
    "        \"aren't\": \"are not\",\n",
    "        \"can't\": \"cannot\",\n",
    "        \"can't've\": \"cannot have\",\n",
    "        \"'cause\": \"because\",\n",
    "        \"could've\": \"could have\",\n",
    "        \"couldn't\": \"could not\",\n",
    "        \"couldn't've\": \"could not have\",\n",
    "        \"didn't\": \"did not\",\n",
    "        \"doesn't\": \"does not\",\n",
    "        \"don't\": \"do not\",\n",
    "        \"hadn't\": \"had not\",\n",
    "        \"hadn't've\": \"had not have\",\n",
    "        \"hasn't\": \"has not\",\n",
    "        \"haven't\": \"have not\",\n",
    "        \"he'd\": \"he would\",\n",
    "        \"he'd've\": \"he would have\",\n",
    "        \"he'll\": \"he will\",\n",
    "        \"he's\": \"he is\",\n",
    "        \"how'd\": \"how did\",\n",
    "        \"how'll\": \"how will\",\n",
    "        \"how's\": \"how is\",\n",
    "        \"i'd\": \"i would\",\n",
    "        \"i'll\": \"i will\",\n",
    "        \"i'm\": \"i am\",\n",
    "        \"i've\": \"i have\",\n",
    "        \"isn't\": \"is not\",\n",
    "        \"it'd\": \"it would\",\n",
    "        \"it'll\": \"it will\",\n",
    "        \"it's\": \"it is\",\n",
    "        \"let's\": \"let us\",\n",
    "        \"ma'am\": \"madam\",\n",
    "        \"might've\": \"might have\",\n",
    "        \"mightn't\": \"might not\",\n",
    "        \"must've\": \"must have\",\n",
    "        \"mustn't\": \"must not\",\n",
    "        \"needn't\": \"need not\",\n",
    "        \"shan't\": \"shall not\",\n",
    "        \"she'd\": \"she would\",\n",
    "        \"she'll\": \"she will\",\n",
    "        \"she's\": \"she is\",\n",
    "        \"should've\": \"should have\",\n",
    "        \"shouldn't\": \"should not\",\n",
    "        \"that'd\": \"that would\",\n",
    "        \"that's\": \"that is\",\n",
    "        \"there's\": \"there is\",\n",
    "        \"they'd\": \"they would\",\n",
    "        \"they'll\": \"they will\",\n",
    "        \"they're\": \"they are\",\n",
    "        \"they've\": \"they have\",\n",
    "        \"wasn't\": \"was not\",\n",
    "        \"we'd\": \"we would\",\n",
    "        \"we'll\": \"we will\",\n",
    "        \"we're\": \"we are\",\n",
    "        \"we've\": \"we have\",\n",
    "        \"weren't\": \"were not\",\n",
    "        \"what're\": \"what are\",\n",
    "        \"what's\": \"what is\",\n",
    "        \"when's\": \"when is\",\n",
    "        \"where'd\": \"where did\",\n",
    "        \"where's\": \"where is\",\n",
    "        \"who'll\": \"who will\",\n",
    "        \"who's\": \"who is\",\n",
    "        \"won't\": \"will not\",\n",
    "        \"wouldn't\": \"would not\",\n",
    "        \"you'd\": \"you would\",\n",
    "        \"you'll\": \"you will\",\n",
    "        \"you're\": \"you are\",\n",
    "        \"you've\": \"you have\"\n",
    "    }\n",
    "    for contraction, expansion in contractions.items():\n",
    "        text = text.replace(contraction, expansion)\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # Remove punctuations\n",
    "    text = re.sub(r'\\p{P}+', '', text)\n",
    "    \n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Tokenization\n",
    "    tokens = text.split()\n",
    "    \n",
    "    # Remove stopwords and lemmatize\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # Reconstruct the text\n",
    "    text = ' '.join(tokens)\n",
    "    \n",
    "    return text\n",
    "\n",
    "df['TEXT'] = df['TEXT'].apply(advanced_transform_text)\n",
    "\n",
    "\n",
    "\n",
    "print(' .:. Data Transformation, Stemming and Tokenization are Done .:.')\n",
    "#This function includes:\n",
    "#Lowercasing\n",
    "#Expanding contractions\n",
    "#Removing URLs\n",
    "#Removing punctuations\n",
    "#Removing numbers\n",
    "#Tokenization\n",
    "#Removing stopwords\n",
    "#Lemmatization\n",
    "# can further expand or modify this function based on the specific requirements of your dataset and problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91681f98",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67df4707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".:. Feature Extraction is Done .:.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the LIWC dataset\n",
    "DP_df = pd.read_csv('LIWC-Dictionary.csv')\n",
    "liwc_corpus = DP_df['DicTerm'].dropna().tolist()  # Drop any NaN values and convert to a list\n",
    "\n",
    "# Initialize and fit the tokenizer\n",
    "tokenizer = Tokenizer(oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(liwc_corpus)\n",
    "\n",
    "# Convert texts to sequences of integers\n",
    "sequences = tokenizer.texts_to_sequences(liwc_corpus)\n",
    "\n",
    "# Convert sequences back to words for Word2Vec and FastText training\n",
    "sentences = [[tokenizer.index_word[idx] for idx in seq] for seq in sequences]\n",
    "\n",
    "# Tokenize the text data in the DataFrame\n",
    "df['tokens'] = [word_tokenize(text) for text in df['TEXT']]\n",
    "\n",
    "# Load Google's pre-trained Word2Vec model.\n",
    "# model_w2v_pretrained = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
    "\n",
    "# Load Facebook's pre-trained FastText model (assuming you have the .vec file).\n",
    "model_ft_pretrained = KeyedVectors.load_word2vec_format('Facebook-crawl-300d-2M.vec')\n",
    "\n",
    "# Train Word2Vec and FastText from scratch using the LIWC text corpus\n",
    "# model_w2v_trained = Word2Vec(sentences, vector_size=300, window=5, min_count=1, workers=4)\n",
    "model_ft_trained = FastText(sentences, vector_size=300, window=3, min_count=1, workers=4)\n",
    "\n",
    "\n",
    "# Convert tokens to vectors\n",
    "def get_vector(tokens, model):\n",
    "    if isinstance(model, KeyedVectors):  # Pre-trained models\n",
    "        vector_list = [model[word] for word in tokens if word in model.key_to_index]\n",
    "    else:  # Trained from scratch models\n",
    "        vector_list = [model.wv[word] for word in tokens if word in model.wv.key_to_index]\n",
    "    return np.mean(vector_list, axis=0) if vector_list else np.zeros(300)  # Assuming vector_size=300\n",
    "\n",
    "# Assuming df is your main dataframe with a 'tokens' column\n",
    "# Using pre-trained models\n",
    "# df['w2v_pretrained'] = df['tokens'].apply(lambda x: get_vector(x, model_w2v_pretrained))\n",
    "df['ft_pretrained'] = df['tokens'].apply(lambda x: get_vector(x, model_ft_pretrained))\n",
    "\n",
    "# Using models trained from scratch\n",
    "# df['w2v_trained'] = df['tokens'].apply(lambda x: get_vector(x, model_w2v_trained))\n",
    "df['ft_trained'] = df['tokens'].apply(lambda x: get_vector(x, model_ft_trained))\n",
    "\n",
    "\n",
    "\n",
    "print(\".:. Feature Extraction is Done .:.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f91ac3b",
   "metadata": {},
   "source": [
    "# Handle Imbalanced Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5c13d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done....\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Display new class counts\n",
    "# Convert the vector columns into a list of arrays\n",
    "# X_w2v = np.array(df['w2v_pretrained'].tolist())\n",
    "X_ft = np.array(df['ft_pretrained'].tolist())\n",
    "\n",
    "# Stack the arrays horizontally\n",
    "# X = np.hstack((X_w2v, X_ft))\n",
    "X = X_ft\n",
    "# Now, you can resample using SMOTE\n",
    "smote = SMOTE(random_state=101)\n",
    "y = df['depression'].values\n",
    "X_res, y_res = smote.fit_resample(X, y)\n",
    "\n",
    "print ('Done....')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ae30f9",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "306ae960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=0.3, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fed24812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model without the embedding layer\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(64, return_sequences=True, input_shape=(X_train.shape[1], 1))))\n",
    "model.add(Bidirectional(LSTM(32)))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a89fbb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Infinite values found in the dataset!\n"
     ]
    }
   ],
   "source": [
    "# Reshape data for LSTM\n",
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "\n",
    "# Check for NaN or Infinite values\n",
    "if np.any(np.isnan(X_train)) or np.any(np.isnan(X_test)):\n",
    "    print(\"NaN values found in the dataset!\")\n",
    "if np.all(np.isfinite(X_train)) or np.all(np.isfinite(X_test)):\n",
    "    print(\"Infinite values found in the dataset!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "971c5317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "10789/10789 [==============================] - 5577s 517ms/step - loss: 0.5957 - accuracy: 0.6744 - val_loss: 0.5923 - val_accuracy: 0.6920\n",
      "Epoch 2/2\n",
      "10789/10789 [==============================] - 5317s 493ms/step - loss: 0.5710 - accuracy: 0.6974 - val_loss: 0.5726 - val_accuracy: 0.6947\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x55e09d280>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=2, batch_size=128, validation_data=(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49bc7f20",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bdde22fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18495/18495 [==============================] - 473s 26ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_pred = [1 if p > 0.5 else 0 for p in y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "50228bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18495/18495 [==============================] - 2302s 124ms/step\n",
      "Precision: 0.7387954334583888\n",
      "Recall: 0.6801259955233102\n",
      "F1 Score: 0.7082477851216425\n",
      "ROC AUC Score: 0.719593806414802\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, auc, precision_recall_curve, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Precision\n",
    "precision = precision_score(y_test, y_pred)\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "\n",
    "# Recall\n",
    "recall = recall_score(y_test, y_pred)\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "\n",
    "# F1-Score\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print(f\"F1-Score: {f1:.4f}\")\n",
    "\n",
    "# AUC-ROC\n",
    "roc_auc = roc_auc_score(y_test, y_pred)\n",
    "print(f\"AUC-ROC: {roc_auc:.4f}\")\n",
    "\n",
    "# AUC-PR\n",
    "precision_curve, recall_curve, _ = precision_recall_curve(y_test, y_pred)\n",
    "auc_pr = auc(recall_curve, precision_curve)\n",
    "print(f\"AUC-PR: {auc_pr:.4f}\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
